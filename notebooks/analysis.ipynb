{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Allstate Claims Severity - Final Master Edition\n",
                "\n",
                "## 1. Import Libraries / ライブラリのインポート\n",
                "Import necessary libraries (Pandas, NumPy, Matplotlib, Seaborn).\n",
                "必要なライブラリを読み込みます。日本語フォント対応もここで行います。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import os\n",
                "\n",
                "# Install and import Japanese font support\n",
                "# 日本語フォント対応ライブラリのインストールとインポート\n",
                "!pip install japanize-matplotlib\n",
                "import japanize_matplotlib\n",
                "\n",
                "%matplotlib inline\n",
                "pd.set_option('display.max_columns', None)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Data / データの読み込み\n",
                "Load the CSV files. Handling paths for both Kaggle and local environment.\n",
                "CSVファイルを読み込みます。Kaggle環境とローカル環境の両方に対応できるようにパスを自動判定します。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Determine the file path / ファイルパスの判定\n",
                "if os.path.exists('/kaggle/input/allstate-claims-severity/train.csv'):\n",
                "    base_path = '/kaggle/input/allstate-claims-severity/'\n",
                "    print('Running on Kaggle (Kaggle環境で実行中)')\n",
                "else:\n",
                "    base_path = '../input/'\n",
                "    print('Running Locally (ローカル環境で実行中)')\n",
                "\n",
                "train = pd.read_csv(base_path + 'train.csv')\n",
                "test = pd.read_csv(base_path + 'test.csv')\n",
                "sample_submission = pd.read_csv(base_path + 'sample_submission.csv')\n",
                "\n",
                "print(f'Train shape (学習データ): {train.shape}')\n",
                "print(f'Test shape (テストデータ): {test.shape}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Data Preprocessing / データ前処理\n",
                "1. Log transform `loss` (to normalize distribution).\n",
                "2. Combine train and test data (for consistent encoding).\n",
                "3. Convert categorical variables to numbers (One-Hot Encoding).\n",
                "\n",
                "1. 目的変数 `loss` を対数変換します（分布を整えるため）。\n",
                "2. 学習データとテストデータを結合します（カテゴリ変数の変換を統一するため）。\n",
                "3. カテゴリ変数を数字（0/1）に変換します（One-Hot Encoding）。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Log transform the target variable / 目的変数を対数変換\n",
                "train['log_loss'] = np.log1p(train['loss'])\n",
                "\n",
                "# Identify Continuous columns / 連続値列の特定\n",
                "cont_features = [col for col in train.columns if 'cont' in col]\n",
                "\n",
                "# Drop 'id' and 'loss' from train for merging / 結合用に不要な列を一時的に削除\n",
                "train_X = train.drop(['id', 'loss', 'log_loss'], axis=1)\n",
                "test_X = test.drop(['id'], axis=1)\n",
                "\n",
                "# Combine train and test / データの結合\n",
                "# (Assigning a split identifier / 後で分割できるようにフラグを立てる)\n",
                "train_X['is_train'] = 1\n",
                "test_X['is_train'] = 0\n",
                "\n",
                "all_data = pd.concat([train_X, test_X], axis=0)\n",
                "print(f'Combined data shape (結合後のサイズ): {all_data.shape}')\n",
                "\n",
                "# One-Hot Encoding (Convert text to numbers) / カテゴリ変数をダミー変数化\n",
                "print('Processing One-Hot Encoding... (変換中...)')\n",
                "all_data = pd.get_dummies(all_data)\n",
                "print(f'Shape after encoding (変換後のサイズ): {all_data.shape}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Split Data for General Models / 全体モデル用のデータ分割\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import xgboost as xgb\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Split back to train and test / 再び学習用とテスト用に分割\n",
                "X_train_all = all_data[all_data['is_train'] == 1].drop(['is_train'], axis=1)\n",
                "X_test_final = all_data[all_data['is_train'] == 0].drop(['is_train'], axis=1)\n",
                "\n",
                "y_train_all = train['log_loss']\n",
                "\n",
                "# Split train data for validation (80% train, 20% validation) / 検証用にデータを8:2に分割\n",
                "X_train, X_val, y_train, y_val = train_test_split(X_train_all, y_train_all, test_size=0.2, random_state=42)\n",
                "\n",
                "# Identify Category Columns (excluding cont features) for later use\n",
                "# 後で「カテゴリ専門モデル」を作るために、カテゴリ列がどれか特定しておきます\n",
                "all_columns = X_train.columns\n",
                "cat_col_names = [c for c in all_columns if c not in cont_features]\n",
                "print(f'Continuous Features: {len(cont_features)}, Categorical Features: {len(cat_col_names)}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                "# Model 1: XGBoost (All Features) / 全データXGBoost\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize XGBoost Regressor / モデルの定義\n",
                "model_xgb = xgb.XGBRegressor(\n",
                "    n_estimators=500,\n",
                "    learning_rate=0.05,\n",
                "    max_depth=6,\n",
                "    random_state=42,\n",
                "    n_jobs=-1,\n",
                "    tree_method='hist',\n",
                "    device='cuda',\n",
                "    early_stopping_rounds=50\n",
                ")\n",
                "\n",
                "print('Training XGBoost (All Features)...')\n",
                "model_xgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
                "\n",
                "# Save Predictions\n",
                "pred_xgb_log = model_xgb.predict(X_test_final)\n",
                "pred_xgb = np.expm1(pred_xgb_log)\n",
                "val_pred_xgb_log = model_xgb.predict(X_val)\n",
                "val_pred_xgb = np.expm1(val_pred_xgb_log)\n",
                "print('XGBoost Finished')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                "# Model 2: LightGBM (All Features) / 全データLightGBM\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import lightgbm as lgb\n",
                "\n",
                "model_lgb = lgb.LGBMRegressor(\n",
                "    n_estimators=1000,\n",
                "    learning_rate=0.05,\n",
                "    random_state=42,\n",
                "    n_jobs=-1,\n",
                "    device='gpu'\n",
                ")\n",
                "\n",
                "print('Training LightGBM (All Features)...')\n",
                "model_lgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='rmse')\n",
                "\n",
                "# Save Predictions\n",
                "pred_lgb_log = model_lgb.predict(X_test_final)\n",
                "pred_lgb = np.expm1(pred_lgb_log)\n",
                "val_pred_lgb_log = model_lgb.predict(X_val)\n",
                "val_pred_lgb = np.expm1(val_pred_lgb_log)\n",
                "print('LightGBM Finished')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                "# Model 3: Neural Network (All Features) / 全データニューラルネット\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.preprocessing import StandardScaler\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
                "from tensorflow.keras.optimizers import Adam\n",
                "from tensorflow.keras.callbacks import EarlyStopping\n",
                "\n",
                "# Scale for NN\n",
                "X_train_nn = X_train.copy()\n",
                "X_val_nn = X_val.copy()\n",
                "X_test_nn = X_test_final.copy()\n",
                "\n",
                "scaler = StandardScaler()\n",
                "X_train_nn[cont_features] = scaler.fit_transform(X_train_nn[cont_features])\n",
                "X_val_nn[cont_features] = scaler.transform(X_val_nn[cont_features])\n",
                "X_test_nn[cont_features] = scaler.transform(X_test_nn[cont_features])\n",
                "\n",
                "def create_model(input_dim):\n",
                "    model = Sequential([\n",
                "        Dense(256, activation='relu', input_dim=input_dim),\n",
                "        BatchNormalization(),\n",
                "        Dropout(0.3),\n",
                "        Dense(128, activation='relu'),\n",
                "        BatchNormalization(),\n",
                "        Dropout(0.3),\n",
                "        Dense(64, activation='relu'),\n",
                "        Dense(1)\n",
                "    ])\n",
                "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
                "    return model\n",
                "\n",
                "print('Training Neural Network (All Features)...')\n",
                "model_nn = create_model(X_train_nn.shape[1])\n",
                "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
                "\n",
                "model_nn.fit(\n",
                "    X_train_nn, y_train,\n",
                "    validation_data=(X_val_nn, y_val),\n",
                "    epochs=50,\n",
                "    batch_size=256,\n",
                "    callbacks=[early_stopping],\n",
                "    verbose=0\n",
                ")\n",
                "\n",
                "# Save Predictions\n",
                "pred_nn_log = model_nn.predict(X_test_nn).flatten()\n",
                "pred_nn = np.expm1(pred_nn_log)\n",
                "val_pred_nn_log = model_nn.predict(X_val_nn).flatten()\n",
                "val_pred_nn = np.expm1(val_pred_nn_log)\n",
                "print('Neural Network Finished')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                "# Expert Models (Diversity Strategy) / 専門家モデルの追加\n",
                "As per your brilliant idea, we create \"Specialist\" models to ensure diversity.\n",
                "あなたのアイデア通り、「専門家AI」を作ることで、視点の多様性を確保します。\n",
                "\n",
                "1. **Model 4: NN (Continuous Only)** -> Focuses only on math/numbers.\n",
                "2. **Model 5: XGB (Categorical Only)** -> Focuses only on types/categories.\n",
                "\n",
                "1. **モデル4: 数値専門ニューラルネット** -> cont列しか見せません。\n",
                "2. **モデル5: カテゴリ専門XGBoost** -> cat列しか見せません。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "### Model 4: Continuous Only Neural Network ###\n",
                "# Use only 'cont_features' (already scaled in X_train_nn)\n",
                "\n",
                "X_train_cont = X_train_nn[cont_features]\n",
                "X_val_cont = X_val_nn[cont_features]\n",
                "X_test_cont = X_test_nn[cont_features]\n",
                "\n",
                "print('Training Expert Model 4: NN (Continuous Only)...')\n",
                "model_nn_cont = create_model(X_train_cont.shape[1]) # Create same model structure\n",
                "model_nn_cont.fit(\n",
                "    X_train_cont, y_train,\n",
                "    validation_data=(X_val_cont, y_val),\n",
                "    epochs=50,\n",
                "    batch_size=256,\n",
                "    callbacks=[early_stopping],\n",
                "    verbose=0\n",
                ")\n",
                "\n",
                "# Save Predictions\n",
                "pred_nn_cont_log = model_nn_cont.predict(X_test_cont).flatten()\n",
                "pred_nn_cont = np.expm1(pred_nn_cont_log)\n",
                "val_pred_nn_cont_log = model_nn_cont.predict(X_val_cont).flatten()\n",
                "val_pred_nn_cont = np.expm1(val_pred_nn_cont_log)\n",
                "print('Expert Model 4 Finished')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "### Model 5: Categorical Only XGBoost ###\n",
                "# Use only 'cat_col_names'\n",
                "\n",
                "X_train_cat = X_train[cat_col_names]\n",
                "X_val_cat = X_val[cat_col_names]\n",
                "X_test_cat = X_test_final[cat_col_names]\n",
                "\n",
                "print('Training Expert Model 5: XGBoost (Categorical Only)...')\n",
                "model_xgb_cat = xgb.XGBRegressor(\n",
                "    n_estimators=500,\n",
                "    learning_rate=0.05,\n",
                "    max_depth=6,\n",
                "    random_state=42,\n",
                "    n_jobs=-1,\n",
                "    tree_method='hist',\n",
                "    device='cuda',\n",
                "    early_stopping_rounds=50\n",
                ")\n",
                "\n",
                "model_xgb_cat.fit(X_train_cat, y_train, eval_set=[(X_val_cat, y_val)], verbose=False)\n",
                "\n",
                "# Save Predictions\n",
                "pred_xgb_cat_log = model_xgb_cat.predict(X_test_cat)\n",
                "pred_xgb_cat = np.expm1(pred_xgb_cat_log)\n",
                "val_pred_xgb_cat_log = model_xgb_cat.predict(X_val_cat)\n",
                "val_pred_xgb_cat = np.expm1(val_pred_xgb_cat_log)\n",
                "print('Expert Model 5 Finished')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. Final 5-Model Optimization / 5モデルの最適化\n",
                "We now mix 5 models: \n",
                "1. XGB (All)\n",
                "2. LGB (All)\n",
                "3. NN (All)\n",
                "4. NN (Continuous Specialist)\n",
                "5. XGB (Categorical Specialist)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scipy.optimize import minimize\n",
                "from sklearn.metrics import mean_absolute_error\n",
                "\n",
                "y_true = np.expm1(y_val)\n",
                "\n",
                "# Collect Validation Predictions\n",
                "V1 = val_pred_xgb\n",
                "V2 = val_pred_lgb\n",
                "V3 = val_pred_nn\n",
                "V4 = val_pred_nn_cont\n",
                "V5 = val_pred_xgb_cat\n",
                "\n",
                "# Collect Test Predictions\n",
                "T1 = pred_xgb\n",
                "T2 = pred_lgb\n",
                "T3 = pred_nn\n",
                "T4 = pred_nn_cont\n",
                "T5 = pred_xgb_cat\n",
                "\n",
                "def loss_func(weights):\n",
                "    final_pred = (weights[0]*V1) + (weights[1]*V2) + (weights[2]*V3) + (weights[3]*V4) + (weights[4]*V5)\n",
                "    return mean_absolute_error(y_true, final_pred)\n",
                "\n",
                "# Initial Guess (0.2 each)\n",
                "init_weights = [0.2] * 5\n",
                "constraints = ({'type': 'eq', 'fun': lambda w: 1 - sum(w)})\n",
                "bounds = [(0, 1)] * 5\n",
                "\n",
                "print('Running 5-Model Optimization...')\n",
                "res = minimize(loss_func, init_weights, method='SLSQP', bounds=bounds, constraints=constraints)\n",
                "\n",
                "print('Optimal Weights (最適な重み):')\n",
                "print(f'1. XGB (All)    : {res.x[0]:.4f}')\n",
                "print(f'2. LGB (All)    : {res.x[1]:.4f}')\n",
                "print(f'3. NN (All)     : {res.x[2]:.4f}')\n",
                "print(f'4. NN (ContOnly): {res.x[3]:.4f}')\n",
                "print(f'5. XGB (CatOnly): {res.x[4]:.4f}')\n",
                "print(f'Best Validation MAE: {res.fun:.4f}')\n",
                "\n",
                "# Apply weights to Test\n",
                "pred_ensemble_final = (res.x[0]*T1) + (res.x[1]*T2) + (res.x[2]*T3) + (res.x[3]*T4) + (res.x[4]*T5)\n",
                "\n",
                "submission_final = pd.DataFrame({\n",
                "    'id': test['id'],\n",
                "    'loss': pred_ensemble_final\n",
                "})\n",
                "\n",
                "submission_final.to_csv('submission_expert_5models.csv', index=False)\n",
                "print('submission_expert_5models.csv created!')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}