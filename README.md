# Allstate Claims Severity - Advanced Ensemble Regressor
**Kaggle Competition Project**

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![XGBoost](https://img.shields.io/badge/Model-XGBoost-green.svg)
![LightGBM](https://img.shields.io/badge/Model-LightGBM-orange.svg)
![Keras](https://img.shields.io/badge/DL-Keras%2FTensorFlow-red.svg)
![Optimization](https://img.shields.io/badge/Technique-Nelder--Mead-purple.svg)

## ğŸ“Œ Project Overview / ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦
Developed a high-performance regression model to predict the "loss" (claim severity) for Allstate Insurance.
Combining Gradient Boosting Trees (XGBoost, LightGBM) and Deep Learning (Neural Networks), achieved a robust ensemble model using automated weight optimization (Nelder-Mead/SLSQP).

Allstateï¼ˆä¿é™ºä¼šç¤¾ï¼‰ã®æå®³é¡ï¼ˆlossï¼‰ã‚’äºˆæ¸¬ã™ã‚‹å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã€‚
æ±ºå®šæœ¨ãƒ¢ãƒ‡ãƒ«ï¼ˆXGBoost, LightGBMï¼‰ã¨ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆNeural Networkï¼‰ã‚’çµ„ã¿åˆã‚ã›ã€ã•ã‚‰ã«æ•°å­¦çš„æœ€é©åŒ–ï¼ˆscipy.optimizeï¼‰ã‚’ç”¨ã„ã¦ã€Œæœ€é©ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿ã€ã‚’è‡ªå‹•ç®—å‡ºã™ã‚‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè£…ã—ã¾ã—ãŸã€‚

## ğŸ† Key Achievements / æˆæœ
- **Private Score (MAE):** 1129.77 (Approx. Top 35% / 3000 teams)
- **Improvement:** Reduced error by ~17 points from baseline (1146 -> 1129).
- **Technique:** Implemented a robust "3-Model Stacked Ensemble" that outperformed single models and simple averaging.

## ğŸ› ï¸ Architecture / ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```mermaid
graph LR
    Data["Raw Data"] --> Pre["Preprocessing<br>(Log Transform / One-Hot)"]
    
    Pre --> XGB["XGBoost<br>(Tree Logic)"]
    Pre --> LGB["LightGBM<br>(Speed & Accuracy)"]
    Pre --> NN["Neural Network<br>(Non-linear / Scaled)"]
    
    XGB --> Pred1["Prediction 1"]
    LGB --> Pred2["Prediction 2"]
    NN  --> Pred3["Prediction 3"]
    
    Pred1 --> Opt["Optimizer<br>(SLSQP / Nelder-Mead)"]
    Pred2 --> Opt
    Pred3 --> Opt
    
    Opt --> Final["Weighted Average<br>(Ensemble)"]
    Final --> Sub["Submission.csv<br>(MAE: 1129.77)"]
    
    style Opt fill:#f9f,stroke:#333,stroke-width:2px
    style Final fill:#bfb,stroke:#333,stroke-width:2px
```

## ğŸ’» Technical Highlights / æŠ€è¡“çš„ãƒã‚¤ãƒ©ã‚¤ãƒˆ

### 1. Hybrid Modeling (ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ»ãƒ¢ãƒ‡ãƒªãƒ³ã‚°)
- **Diversity:** Combined "Tree-based" models (strong at categorical splits) with "Neural Networks" (strong at continuous scaling) to capture different data patterns.
- ã€Œæ±ºå®šæœ¨ãŒå¾—æ„ãªè«–ç†çš„æ¨è«–ã€ã¨ã€Œãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãŒå¾—æ„ãªæ•°å­¦çš„æ¨è«–ã€ã‚’çµ„ã¿åˆã‚ã›ã€å¤šæ§˜æ€§ã‚’ç¢ºä¿ã—ã¾ã—ãŸã€‚

### 2. Automated Weight Optimization (é‡ã¿æœ€é©åŒ–)
- Instead of manual guessing, used `scipy.optimize.minimize` to mathematically find the "Golden Ratio" of model weights based on validation data (OOF).
- æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ï¼ˆValidationï¼‰ã«å¯¾ã™ã‚‹èª¤å·®ãŒæœ€å°ã«ãªã‚‹ã‚ˆã†ãªé‡ã¿é…åˆ†ï¼ˆä¾‹: XGB 25%, NN 46%...ï¼‰ã‚’è‡ªå‹•è¨ˆç®—ã—ã€äººçš„ãƒã‚¤ã‚¢ã‚¹ã‚’æ’é™¤ã—ã¾ã—ãŸã€‚

### 3. GPU Acceleration (GPUé«˜é€ŸåŒ–)
- Enabled `device='cuda'` for XGBoost and `device='gpu'` for LightGBM/Keras to accelerate training on simple hardware.
- Kaggleç’°å¢ƒã®GPUã‚’ãƒ•ãƒ«æ´»ç”¨ã—ã€é«˜é€Ÿãªå®Ÿé¨“ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿç¾ã—ã¾ã—ãŸã€‚

## ğŸ“‚ Code Structure
- `analysis.ipynb`: Main experimentation notebook (EDA, Training, Optimization).
- `eda_backup.py`: Production-ready backup script.
